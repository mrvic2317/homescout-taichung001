"""æ”¿åºœé–‹æ”¾è³‡æ–™è‡ªå‹•ä¸‹è¼‰æœå‹™.

æ”¯æ´ï¼š
- å¤šåŸå¸‚è³‡æ–™ä¾†æºé…ç½®
- 7 å¤©å¿«å–æ©Ÿåˆ¶
- è‡ªå‹•ä¸‹è¼‰å’Œæ›´æ–°
- éŒ¯èª¤è™•ç†å’Œé™ç´šç­–ç•¥
"""

import asyncio
import logging
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Optional
from urllib.parse import urljoin

import aiohttp

logger = logging.getLogger(__name__)

# è³‡æ–™ä¾†æºé…ç½®
# æ³¨æ„ï¼šä»¥ä¸‹ URL ç‚ºç¯„ä¾‹ï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€æ›¿æ›ç‚ºçœŸå¯¦çš„æ”¿åºœé–‹æ”¾è³‡æ–™é€£çµ
DATA_SOURCES = {
    "taichung": {
        "name": "å°ä¸­å¸‚",
        "url": "https://data.gov.tw/api/v2/rest/dataset/export?dataset_id=XXXXX",
        "filename": "taichung_prices.csv",
        "description": "å°ä¸­å¸‚ä¸å‹•ç”¢è²·è³£å¯¦åƒ¹ç™»éŒ„",
    },
    "taipei": {
        "name": "å°åŒ—å¸‚",
        "url": "https://data.gov.tw/api/v2/rest/dataset/export?dataset_id=XXXXX",
        "filename": "taipei_prices.csv",
        "description": "å°åŒ—å¸‚ä¸å‹•ç”¢è²·è³£å¯¦åƒ¹ç™»éŒ„",
    },
    "new_taipei": {
        "name": "æ–°åŒ—å¸‚",
        "url": "https://data.gov.tw/api/v2/rest/dataset/export?dataset_id=XXXXX",
        "filename": "new_taipei_prices.csv",
        "description": "æ–°åŒ—å¸‚ä¸å‹•ç”¢è²·è³£å¯¦åƒ¹ç™»éŒ„",
    },
    "kaohsiung": {
        "name": "é«˜é›„å¸‚",
        "url": "https://data.gov.tw/api/v2/rest/dataset/export?dataset_id=XXXXX",
        "filename": "kaohsiung_prices.csv",
        "description": "é«˜é›„å¸‚ä¸å‹•ç”¢è²·è³£å¯¦åƒ¹ç™»éŒ„",
    },
}

# å¿«å–æœ‰æ•ˆæœŸï¼ˆå¤©æ•¸ï¼‰
CACHE_VALIDITY_DAYS = 7

# è³‡æ–™ç›®éŒ„
DATA_DIR = Path(__file__).parent.parent.parent / "data"


class DataDownloader:
    """æ”¿åºœé–‹æ”¾è³‡æ–™ä¸‹è¼‰å™¨."""

    def __init__(self, data_dir: Path = DATA_DIR):
        """åˆå§‹åŒ–ä¸‹è¼‰å™¨.

        Args:
            data_dir: è³‡æ–™å­˜æ”¾ç›®éŒ„
        """
        self.data_dir = data_dir
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def _get_file_path(self, city_key: str) -> Path:
        """å–å¾—åŸå¸‚è³‡æ–™æª”æ¡ˆè·¯å¾‘.

        Args:
            city_key: åŸå¸‚éµå€¼ï¼ˆä¾‹å¦‚ï¼štaichungï¼‰

        Returns:
            æª”æ¡ˆå®Œæ•´è·¯å¾‘
        """
        source = DATA_SOURCES.get(city_key)
        if not source:
            raise ValueError(f"ä¸æ”¯æ´çš„åŸå¸‚ï¼š{city_key}")

        return self.data_dir / source["filename"]

    def _is_cache_valid(self, file_path: Path) -> bool:
        """æª¢æŸ¥å¿«å–æ˜¯å¦æœ‰æ•ˆ.

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘

        Returns:
            True å¦‚æœå¿«å–æœ‰æ•ˆï¼ˆæª”æ¡ˆå­˜åœ¨ä¸”æœªéæœŸï¼‰
        """
        if not file_path.exists():
            logger.info(f"å¿«å–æª”æ¡ˆä¸å­˜åœ¨ | path={file_path}")
            return False

        # æª¢æŸ¥æª”æ¡ˆä¿®æ”¹æ™‚é–“
        mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
        age = datetime.now() - mtime
        is_valid = age.days < CACHE_VALIDITY_DAYS

        if is_valid:
            logger.info(
                f"å¿«å–ä»æœ‰æ•ˆ | path={file_path.name} | age={age.days}å¤© | "
                f"valid_until={CACHE_VALIDITY_DAYS}å¤©"
            )
        else:
            logger.info(
                f"å¿«å–å·²éæœŸ | path={file_path.name} | age={age.days}å¤© | "
                f"expired_after={CACHE_VALIDITY_DAYS}å¤©"
            )

        return is_valid

    async def download_city_data(
        self, city_key: str, force: bool = False
    ) -> Optional[Path]:
        """ä¸‹è¼‰åŸå¸‚è³‡æ–™.

        Args:
            city_key: åŸå¸‚éµå€¼ï¼ˆä¾‹å¦‚ï¼štaichungï¼‰
            force: æ˜¯å¦å¼·åˆ¶é‡æ–°ä¸‹è¼‰ï¼ˆå¿½ç•¥å¿«å–ï¼‰

        Returns:
            ä¸‹è¼‰å¾Œçš„æª”æ¡ˆè·¯å¾‘ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        source = DATA_SOURCES.get(city_key)
        if not source:
            logger.error(f"âŒ ä¸æ”¯æ´çš„åŸå¸‚ | city={city_key}")
            return None

        file_path = self._get_file_path(city_key)

        # æª¢æŸ¥å¿«å–
        if not force and self._is_cache_valid(file_path):
            logger.info(f"âœ… ä½¿ç”¨å¿«å–è³‡æ–™ | city={source['name']} | path={file_path.name}")
            return file_path

        # ä¸‹è¼‰è³‡æ–™
        logger.info(f"ğŸ”„ é–‹å§‹ä¸‹è¼‰è³‡æ–™ | city={source['name']} | url={source['url']}")

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    source["url"], timeout=aiohttp.ClientTimeout(total=300)
                ) as response:
                    if response.status != 200:
                        logger.error(
                            f"âŒ ä¸‹è¼‰å¤±æ•— | city={source['name']} | "
                            f"status={response.status}"
                        )
                        # å¦‚æœä¸‹è¼‰å¤±æ•—ä¸”æœ‰èˆŠå¿«å–ï¼Œä½¿ç”¨èˆŠå¿«å–
                        if file_path.exists():
                            logger.warning(
                                f"âš ï¸ ä¸‹è¼‰å¤±æ•—ï¼Œä½¿ç”¨èˆŠå¿«å– | city={source['name']} | "
                                f"path={file_path.name}"
                            )
                            return file_path
                        return None

                    # è®€å–å…§å®¹
                    content = await response.read()

                    # å„²å­˜æª”æ¡ˆ
                    with open(file_path, "wb") as f:
                        f.write(content)

                    # è¨˜éŒ„æˆåŠŸ
                    size_mb = len(content) / (1024 * 1024)
                    logger.info(
                        f"âœ… ä¸‹è¼‰æˆåŠŸ | city={source['name']} | "
                        f"size={size_mb:.2f}MB | path={file_path.name}"
                    )

                    return file_path

        except asyncio.TimeoutError:
            logger.error(f"âŒ ä¸‹è¼‰è¶…æ™‚ | city={source['name']} | timeout=300ç§’")
        except aiohttp.ClientError as e:
            logger.error(f"âŒ ç¶²è·¯é€£ç·šéŒ¯èª¤ | city={source['name']} | error={e}")
        except Exception as e:
            logger.error(f"âŒ ä¸‹è¼‰å¤±æ•— | city={source['name']} | error={e}")

        # ä¸‹è¼‰å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨èˆŠå¿«å–
        if file_path.exists():
            logger.warning(
                f"âš ï¸ ä¸‹è¼‰å¤±æ•—ï¼Œä½¿ç”¨èˆŠå¿«å– | city={source['name']} | "
                f"path={file_path.name}"
            )
            return file_path

        return None

    async def download_all_cities(self, force: bool = False) -> Dict[str, Optional[Path]]:
        """ä¸‹è¼‰æ‰€æœ‰åŸå¸‚è³‡æ–™.

        Args:
            force: æ˜¯å¦å¼·åˆ¶é‡æ–°ä¸‹è¼‰ï¼ˆå¿½ç•¥å¿«å–ï¼‰

        Returns:
            åŸå¸‚éµå€¼ -> æª”æ¡ˆè·¯å¾‘çš„å­—å…¸
        """
        logger.info(f"ğŸš€ é–‹å§‹ä¸‹è¼‰æ‰€æœ‰åŸå¸‚è³‡æ–™ | cities={len(DATA_SOURCES)} | force={force}")

        tasks = [
            self.download_city_data(city_key, force=force)
            for city_key in DATA_SOURCES.keys()
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ•´ç†çµæœ
        downloads = {}
        for city_key, result in zip(DATA_SOURCES.keys(), results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ä¸‹è¼‰å¤±æ•— | city={city_key} | error={result}")
                downloads[city_key] = None
            else:
                downloads[city_key] = result

        # çµ±è¨ˆ
        success_count = sum(1 for path in downloads.values() if path is not None)
        logger.info(
            f"ğŸ“Š ä¸‹è¼‰å®Œæˆ | total={len(DATA_SOURCES)} | "
            f"success={success_count} | failed={len(DATA_SOURCES) - success_count}"
        )

        return downloads

    async def ensure_city_data(self, city_key: str) -> Optional[Path]:
        """ç¢ºä¿åŸå¸‚è³‡æ–™å¯ç”¨ï¼ˆå„ªå…ˆä½¿ç”¨å¿«å–ï¼Œå¿…è¦æ™‚ä¸‹è¼‰ï¼‰.

        é€™æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œæœƒï¼š
        1. æª¢æŸ¥å¿«å–æ˜¯å¦æœ‰æ•ˆ
        2. è‹¥å¿«å–æœ‰æ•ˆï¼Œç›´æ¥è¿”å›
        3. è‹¥å¿«å–ç„¡æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œä¸‹è¼‰æ–°è³‡æ–™

        Args:
            city_key: åŸå¸‚éµå€¼ï¼ˆä¾‹å¦‚ï¼štaichungï¼‰

        Returns:
            è³‡æ–™æª”æ¡ˆè·¯å¾‘ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        return await self.download_city_data(city_key, force=False)

    def get_available_cities(self) -> Dict[str, str]:
        """å–å¾—æ‰€æœ‰å¯ç”¨åŸå¸‚æ¸…å–®.

        Returns:
            åŸå¸‚éµå€¼ -> åŸå¸‚åç¨±çš„å­—å…¸
        """
        return {key: source["name"] for key, source in DATA_SOURCES.items()}

    def get_cache_info(self, city_key: str) -> Optional[Dict[str, any]]:
        """å–å¾—å¿«å–è³‡è¨Š.

        Args:
            city_key: åŸå¸‚éµå€¼

        Returns:
            åŒ…å«å¿«å–è³‡è¨Šçš„å­—å…¸ï¼Œæª”æ¡ˆä¸å­˜åœ¨æ™‚è¿”å› None
        """
        source = DATA_SOURCES.get(city_key)
        if not source:
            return None

        file_path = self._get_file_path(city_key)
        if not file_path.exists():
            return None

        stat = file_path.stat()
        mtime = datetime.fromtimestamp(stat.st_mtime)
        age = datetime.now() - mtime
        is_valid = age.days < CACHE_VALIDITY_DAYS

        return {
            "city": source["name"],
            "file_path": str(file_path),
            "file_size_mb": stat.st_size / (1024 * 1024),
            "last_modified": mtime.strftime("%Y-%m-%d %H:%M:%S"),
            "age_days": age.days,
            "is_valid": is_valid,
            "expires_in_days": max(0, CACHE_VALIDITY_DAYS - age.days),
        }


# å…¨åŸŸä¸‹è¼‰å™¨å¯¦ä¾‹
_downloader = DataDownloader()


async def download_taichung_data(force: bool = False) -> Optional[Path]:
    """ä¸‹è¼‰å°ä¸­å¸‚è³‡æ–™ï¼ˆä¾¿æ·å‡½å¼ï¼‰.

    Args:
        force: æ˜¯å¦å¼·åˆ¶é‡æ–°ä¸‹è¼‰

    Returns:
        æª”æ¡ˆè·¯å¾‘
    """
    return await _downloader.download_city_data("taichung", force=force)


async def ensure_taichung_data() -> Optional[Path]:
    """ç¢ºä¿å°ä¸­å¸‚è³‡æ–™å¯ç”¨ï¼ˆä¾¿æ·å‡½å¼ï¼‰.

    å„ªå…ˆä½¿ç”¨å®˜æ–¹ä¸‹è¼‰å™¨ï¼ˆçˆ¬å–æœ€æ–°è³‡æ–™ï¼‰ï¼Œå¤±æ•—æ™‚ä½¿ç”¨èˆŠæ–¹æ³•ã€‚

    Returns:
        æª”æ¡ˆè·¯å¾‘
    """
    try:
        # å„ªå…ˆä½¿ç”¨å®˜æ–¹ä¸‹è¼‰å™¨
        from . import official_data_downloader

        logger.info("ä½¿ç”¨å®˜æ–¹ä¸‹è¼‰å™¨ç²å–è³‡æ–™")
        success = await official_data_downloader.ensure_taichung_data()

        if success:
            output_file = DATA_DIR / "taichung_prices.csv"
            if output_file.exists():
                logger.info(f"âœ… å®˜æ–¹è³‡æ–™å·²å°±ç·’ | path={output_file}")
                return output_file

    except ImportError:
        logger.warning("å®˜æ–¹ä¸‹è¼‰å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨èˆŠæ–¹æ³•")
    except Exception as e:
        logger.warning(f"å®˜æ–¹ä¸‹è¼‰å™¨å¤±æ•— | error={e}")

    # é™ç´šï¼šä½¿ç”¨èˆŠæ–¹æ³•
    logger.info("é™ç´šä½¿ç”¨èˆŠä¸‹è¼‰æ–¹æ³•")
    return await _downloader.ensure_city_data("taichung")


async def download_all_cities(force: bool = False) -> Dict[str, Optional[Path]]:
    """ä¸‹è¼‰æ‰€æœ‰åŸå¸‚è³‡æ–™ï¼ˆä¾¿æ·å‡½å¼ï¼‰.

    Args:
        force: æ˜¯å¦å¼·åˆ¶é‡æ–°ä¸‹è¼‰

    Returns:
        åŸå¸‚ -> æª”æ¡ˆè·¯å¾‘çš„å­—å…¸
    """
    return await _downloader.download_all_cities(force=force)


def get_taichung_cache_info() -> Optional[Dict[str, any]]:
    """å–å¾—å°ä¸­å¸‚å¿«å–è³‡è¨Šï¼ˆä¾¿æ·å‡½å¼ï¼‰.

    å„ªå…ˆè¿”å›å®˜æ–¹ä¸‹è¼‰å™¨çš„ç‰ˆæœ¬è³‡è¨Šï¼Œå¤±æ•—æ™‚ä½¿ç”¨èˆŠæ–¹æ³•ã€‚

    Returns:
        å¿«å–è³‡è¨Šå­—å…¸
    """
    try:
        # å„ªå…ˆä½¿ç”¨å®˜æ–¹ä¸‹è¼‰å™¨çš„ç‰ˆæœ¬è³‡è¨Š
        from . import official_data_downloader

        version_info = official_data_downloader.get_version_info()
        if version_info:
            # è½‰æ›æ ¼å¼ä»¥åŒ¹é…èˆŠçš„ä»‹é¢
            output_file = DATA_DIR / "taichung_prices.csv"
            if output_file.exists():
                stat = output_file.stat()
                last_download = version_info.get("last_download")

                if last_download:
                    from datetime import datetime
                    try:
                        mtime = datetime.fromisoformat(last_download)
                        age = datetime.now() - mtime

                        return {
                            "city": "å°ä¸­å¸‚",
                            "file_path": str(output_file),
                            "file_size_mb": stat.st_size / (1024 * 1024),
                            "last_modified": mtime.strftime("%Y-%m-%d %H:%M:%S"),
                            "age_days": age.days,
                            "is_valid": age.days < CACHE_VALIDITY_DAYS,
                            "expires_in_days": max(0, CACHE_VALIDITY_DAYS - age.days),
                            "version": version_info.get("version"),
                            "row_count": version_info.get("row_count"),
                            "source": "official",  # æ¨™è¨˜ä¾†æº
                        }
                    except Exception:
                        pass

    except Exception as e:
        logger.debug(f"ç„¡æ³•å–å¾—å®˜æ–¹ä¸‹è¼‰å™¨è³‡è¨Š | error={e}")

    # é™ç´šï¼šä½¿ç”¨èˆŠæ–¹æ³•
    return _downloader.get_cache_info("taichung")
