"""æ¸¬è©¦å®˜æ–¹è³‡æ–™ä¸‹è¼‰å™¨åŠŸèƒ½."""
import asyncio
import logging
from pathlib import Path
from src.services import official_data_downloader

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')


async def test_1_fetch_version_info():
    """æ¸¬è©¦ 1ï¼šçˆ¬å–æœ€æ–°ç‰ˆæœ¬è³‡è¨Š."""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 1ï¼šçˆ¬å–æœ€æ–°ç‰ˆæœ¬è³‡è¨Š")
    print("="*60)
    print("  ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼šå…§æ”¿éƒ¨æ‰¹æ¬¡ä¸‹è¼‰é€£çµ")

    downloader = official_data_downloader.OfficialDataDownloader()

    try:
        version_info = await downloader.fetch_latest_version_info()

        if version_info:
            print(f"  âœ… æˆåŠŸçˆ¬å–ç‰ˆæœ¬è³‡è¨Š")
            print(f"     ç‰ˆæœ¬ï¼š{version_info['version']}")
            print(f"     ä¸‹è¼‰é€£çµï¼š{version_info['download_url'][:100]}...")
            print(f"     æª”æ¡ˆåç¨±ï¼š{version_info['file_name']}")

            # æª¢æŸ¥æ˜¯å¦ç‚º ZIP æª”æ¡ˆ
            if version_info['file_name'].endswith('.zip'):
                print(f"     âš ï¸ æª”æ¡ˆé¡å‹ï¼šZIPï¼ˆéœ€è¦è§£å£“ç¸®ï¼‰")
        else:
            print(f"  âš ï¸ æœªæ‰¾åˆ°ç‰ˆæœ¬è³‡è¨Šï¼ˆå¯èƒ½æ˜¯ç¶²è·¯å•é¡Œæˆ–ç¶²ç«™çµæ§‹è®Šæ›´ï¼‰")

    except Exception as e:
        print(f"  âŒ çˆ¬å–å¤±æ•—ï¼š{e}")


async def test_2_check_cache():
    """æ¸¬è©¦ 2ï¼šæª¢æŸ¥å¿«å–ç‹€æ…‹."""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 2ï¼šæª¢æŸ¥å¿«å–ç‹€æ…‹")
    print("="*60)

    version_info = official_data_downloader.VersionInfo()

    # æª¢æŸ¥å¿«å–æ˜¯å¦æœ‰æ•ˆ
    is_valid = version_info.is_cache_valid()
    cache_age = version_info.get_cache_age_days()

    print(f"  å¿«å–æœ‰æ•ˆï¼š{'âœ… æ˜¯' if is_valid else 'âŒ å¦'}")

    if cache_age is not None:
        print(f"  å¿«å–å¹´é½¡ï¼š{cache_age} å¤©")
        print(f"  æœ€å¾Œä¸‹è¼‰ï¼š{version_info.get('last_download')}")
        print(f"  ç‰ˆæœ¬ï¼š{version_info.get('version')}")
        print(f"  è³‡æ–™ç­†æ•¸ï¼š{version_info.get('row_count')}")
    else:
        print(f"  å°šç„¡å¿«å–è³‡æ–™")


async def test_3_encoding_detection():
    """æ¸¬è©¦ 3ï¼šç·¨ç¢¼æª¢æ¸¬."""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 3ï¼šæª”æ¡ˆç·¨ç¢¼æª¢æ¸¬")
    print("="*60)

    downloader = official_data_downloader.OfficialDataDownloader()

    # æª¢æŸ¥ç¾æœ‰ CSV æª”æ¡ˆ
    csv_file = Path("data/taichung_prices.csv")

    if csv_file.exists():
        encoding = downloader.detect_encoding(csv_file)
        print(f"  æª”æ¡ˆï¼š{csv_file}")
        print(f"  æª¢æ¸¬åˆ°ç·¨ç¢¼ï¼š{encoding}")
    else:
        print(f"  âš ï¸ æª”æ¡ˆä¸å­˜åœ¨ï¼š{csv_file}")


async def test_4_filter_data():
    """æ¸¬è©¦ 4ï¼šè³‡æ–™éæ¿¾ï¼ˆæ¨¡æ“¬ï¼‰."""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 4ï¼šå°ä¸­å¸‚è³‡æ–™éæ¿¾")
    print("="*60)

    print("  âš ï¸ æ­¤æ¸¬è©¦éœ€è¦å¯¦éš›ä¸‹è¼‰çš„å…¨åœ‹è³‡æ–™")
    print("  å¦‚æœæœ‰ temp_download.csvï¼Œå°‡å˜—è©¦éæ¿¾")

    downloader = official_data_downloader.OfficialDataDownloader()

    temp_file = Path("data/temp_download.csv")
    output_file = Path("data/test_filtered.csv")

    if temp_file.exists():
        success, row_count = downloader.filter_taichung_data(temp_file, output_file)

        if success:
            print(f"  âœ… éæ¿¾æˆåŠŸ | å°ä¸­å¸‚ç­†æ•¸={row_count}")

            if output_file.exists():
                output_file.unlink()  # æ¸…ç†æ¸¬è©¦æª”æ¡ˆ
        else:
            print(f"  âŒ éæ¿¾å¤±æ•—")
    else:
        print(f"  â­ï¸ è·³éæ¸¬è©¦ï¼ˆç„¡æ¸¬è©¦è³‡æ–™ï¼‰")


async def test_5_ensure_data():
    """æ¸¬è©¦ 5ï¼šç¢ºä¿è³‡æ–™å¯ç”¨ï¼ˆå®Œæ•´æµç¨‹ï¼‰."""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 5ï¼šç¢ºä¿è³‡æ–™å¯ç”¨ï¼ˆå®Œæ•´æµç¨‹ï¼‰")
    print("="*60)
    print("  âš ï¸ æ­¤æ¸¬è©¦æœƒå˜—è©¦ä¸‹è¼‰çœŸå¯¦è³‡æ–™ï¼Œè«‹è¬¹æ…åŸ·è¡Œ")
    print("  å¦‚æœå¿«å–æœ‰æ•ˆï¼Œå°‡ç›´æ¥ä½¿ç”¨å¿«å–")

    user_input = input("\n  æ˜¯å¦åŸ·è¡Œæ­¤æ¸¬è©¦ï¼Ÿ(y/N): ")

    if user_input.lower() != 'y':
        print("  â­ï¸ è·³éæ¸¬è©¦")
        return

    try:
        downloader = official_data_downloader.OfficialDataDownloader()

        # å˜—è©¦ç¢ºä¿è³‡æ–™å¯ç”¨
        success = await downloader.ensure_data()

        if success:
            print(f"  âœ… è³‡æ–™å·²å°±ç·’")

            # é¡¯ç¤ºæª”æ¡ˆè³‡è¨Š
            output_file = Path("data/taichung_prices.csv")
            if output_file.exists():
                size_mb = output_file.stat().st_size / (1024 * 1024)
                print(f"     æª”æ¡ˆå¤§å°ï¼š{size_mb:.2f} MB")

            # é¡¯ç¤ºç‰ˆæœ¬è³‡è¨Š
            version_info = official_data_downloader.get_version_info()
            if version_info:
                print(f"     ç‰ˆæœ¬ï¼š{version_info.get('version')}")
                print(f"     è³‡æ–™ç­†æ•¸ï¼š{version_info.get('row_count')}")
        else:
            print(f"  âŒ è³‡æ–™ç„¡æ³•å–å¾—")

    except Exception as e:
        print(f"  âŒ æ¸¬è©¦å¤±æ•—ï¼š{e}")


async def test_6_version_file():
    """æ¸¬è©¦ 6ï¼šç‰ˆæœ¬è³‡è¨Šæª”æ¡ˆ."""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 6ï¼šç‰ˆæœ¬è³‡è¨Šæª”æ¡ˆ (.version_info.json)")
    print("="*60)

    version_file = Path("data/.version_info.json")

    if version_file.exists():
        print(f"  âœ… ç‰ˆæœ¬æª”æ¡ˆå­˜åœ¨")

        import json
        with open(version_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        print(f"     ç‰ˆæœ¬ï¼š{data.get('version')}")
        print(f"     æœ€å¾Œä¸‹è¼‰ï¼š{data.get('last_download')}")
        print(f"     è³‡æ–™ç­†æ•¸ï¼š{data.get('row_count')}")
        print(f"     æª”æ¡ˆå¤§å°ï¼š{data.get('file_size') / (1024*1024):.2f} MB" if data.get('file_size') else "     æª”æ¡ˆå¤§å°ï¼šæœªçŸ¥")

        fields = data.get('fields', [])
        if fields:
            print(f"     æ¬„ä½æ•¸é‡ï¼š{len(fields)}")
            print(f"     æ¬„ä½ç¯„ä¾‹ï¼š{', '.join(fields[:5])}...")
    else:
        print(f"  âš ï¸ ç‰ˆæœ¬æª”æ¡ˆä¸å­˜åœ¨ï¼ˆå°šæœªä¸‹è¼‰è³‡æ–™ï¼‰")


async def test_7_backup_system():
    """æ¸¬è©¦ 7ï¼šå‚™ä»½ç³»çµ±."""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 7ï¼šå‚™ä»½ç³»çµ±")
    print("="*60)

    backup_dir = Path("data/backup")

    if backup_dir.exists():
        backups = list(backup_dir.glob("taichung_prices_*.csv"))

        print(f"  å‚™ä»½ç›®éŒ„å­˜åœ¨ï¼š{backup_dir}")
        print(f"  å‚™ä»½æª”æ¡ˆæ•¸é‡ï¼š{len(backups)}")

        if backups:
            # é¡¯ç¤ºæœ€è¿‘ 3 å€‹å‚™ä»½
            recent_backups = sorted(backups)[-3:]
            print(f"\n  æœ€è¿‘çš„å‚™ä»½ï¼š")
            for backup in recent_backups:
                size_mb = backup.stat().st_size / (1024 * 1024)
                print(f"    - {backup.name} ({size_mb:.2f} MB)")
    else:
        print(f"  âš ï¸ å‚™ä»½ç›®éŒ„ä¸å­˜åœ¨")


async def main():
    """ä¸»æ¸¬è©¦å‡½å¼."""
    print("\n" + "ğŸš€ é–‹å§‹æ¸¬è©¦å®˜æ–¹è³‡æ–™ä¸‹è¼‰å™¨ " + "ğŸš€")

    # 1. çˆ¬å–ç‰ˆæœ¬è³‡è¨Š
    await test_1_fetch_version_info()

    # 2. æª¢æŸ¥å¿«å–ç‹€æ…‹
    await test_2_check_cache()

    # 3. ç·¨ç¢¼æª¢æ¸¬
    await test_3_encoding_detection()

    # 4. è³‡æ–™éæ¿¾ï¼ˆè·³éï¼Œéœ€è¦å¯¦éš›è³‡æ–™ï¼‰
    await test_4_filter_data()

    # 5. ç¢ºä¿è³‡æ–™å¯ç”¨ï¼ˆäº’å‹•å¼ï¼Œéœ€è¦ç”¨æˆ¶ç¢ºèªï¼‰
    await test_5_ensure_data()

    # 6. ç‰ˆæœ¬è³‡è¨Šæª”æ¡ˆ
    await test_6_version_file()

    # 7. å‚™ä»½ç³»çµ±
    await test_7_backup_system()

    print("\n" + "="*60)
    print("ğŸ‰ æ¸¬è©¦å®Œæˆ")
    print("="*60)

    print("\nğŸ’¡ ä½¿ç”¨èªªæ˜ï¼š")
    print("   1. é¦–æ¬¡åŸ·è¡Œæœƒå˜—è©¦ä¸‹è¼‰æœ€æ–°è³‡æ–™ï¼ˆéœ€ç¶²è·¯ï¼‰")
    print("   2. ä¸‹è¼‰æˆåŠŸå¾Œï¼Œè³‡æ–™æœƒå¿«å– 7 å¤©")
    print("   3. èˆŠè³‡æ–™æœƒè‡ªå‹•å‚™ä»½åˆ° data/backup/")
    print("   4. ç‰ˆæœ¬è³‡è¨Šè¨˜éŒ„åœ¨ data/.version_info.json")
    print("   5. å¦‚éœ€å¼·åˆ¶æ›´æ–°ï¼ŒåŸ·è¡Œï¼š")
    print("      python3 -c 'import asyncio; from src.services import official_data_downloader; asyncio.run(official_data_downloader.force_update())'")
    print()


if __name__ == "__main__":
    asyncio.run(main())
